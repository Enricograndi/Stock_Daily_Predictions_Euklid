{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Machine Learning Libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Import Logistic Regression function\n",
    "# Import XGBoost Classifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "#config = tf.compat.v1.ConfigProto(device_count = {'GPU': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary work on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "yahoo_df = pd.read_csv(\"../data/IBM.csv\")\n",
    "yahoo_df = yahoo_df.set_index(\"Date\")\n",
    "\n",
    "X = yahoo_df[['ROC_3', 'Momentum', 'RSI_2', 'ATR_14']]\n",
    "\n",
    "y = yahoo_df[\"Up down\"]\n",
    "\n",
    "# Split in 80/20 the dataframeX\n",
    "X_train_80, X_test, y_train_80, y_test = train_test_split(X, y, test_size = 0.20,\n",
    "                                                          shuffle=False)\n",
    "\n",
    "# Split in 75/25 the remaining 80 %\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_80, y_train_80,\n",
    "                                                      test_size = 0.25, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_norm(df):\n",
    "    #df = data\n",
    "    for column in df:\n",
    "        df[column]=((df[column]-df[column].mean())/df[column].std())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_80 = shift_norm(X_train_80)\n",
    "X_test = shift_norm(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = shift_norm(X_train)\n",
    "X_valid = shift_norm(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Classifier (XGboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost is a library based on GradientBoostingClassifier. Many consider it as one of the best algorithms and, due to its great performance for regression and classification problems, would recommend it as a first choice in many situations. XGBoost has become famous for winning tons of Kaggle competitions, is now used in many industry-application, and is even implemented within machine-learning platforms, such as BigQuery ML.\n",
    "\n",
    "GBoosting is a sequential technique which works on the principle of ensemble. It combines a set of weak learners and delivers improved prediction accuracy. At any instant t, the model outcomes are weighed based on the outcomes of previous instant t-1. The outcomes predicted correctly are given a lower weight and the ones miss-classified are weighted higher. This technique is followed for a classification problem while a similar technique is used for regression.\n",
    "\n",
    "- Pros\n",
    "\n",
    "It is extremely powerful machine learning classifier.\n",
    "Accepts various types of inputs that make it more flexible.\n",
    "It can be used for both regression and classification.\n",
    "It gives you features important for the output.\n",
    "\n",
    "- Cons\n",
    "\n",
    "It takes longer time to train as it canâ€™t be parallelized.\n",
    "More likely to overfit as it obsessed with the wrong output as it learns from past mistakes.\n",
    "In some cases, Tuning is very hard as it has many parameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.94886 - Validation Accuracy: 0.51263\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "XGB = XGBClassifier(random_state = 42)\n",
    "XGB.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_train = XGB.predict(X_train)\n",
    "y_pred_valid = XGB.predict(X_valid)\n",
    "\n",
    "# Compute accuracy\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "valid_acc = accuracy_score(y_valid, y_pred_valid)\n",
    "print(\"Train Accuracy: {:.5f} - Validation Accuracy: {:.5f}\".format(train_acc, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters very slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "params = { 'max_depth': [3,6,10],\n",
    "           'learning_rate': [0.01, 0.05, 0.1],\n",
    "           'n_estimators': [100, 500, 1000],\n",
    "           'colsample_bytree': [0.3, 0.7]}\n",
    "\n",
    "XGB = XGBClassifier(seed = 42)\n",
    "best_XGB = GridSearchCV(estimator=XGB, \n",
    "                   param_grid=params,\n",
    "                   scoring='accuracy', \n",
    "                   verbose=1)\n",
    "best_XGB.fit(X_train, y_train)\n",
    "print(\"Best parameters:\", best_XGB.best_params_)\n",
    "#print(\"Lowest RMSE: \", (-clf.best_score_)**(1/2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.49626 \n"
     ]
    }
   ],
   "source": [
    "# Fit \n",
    "XGB = XGBClassifier(colsample_bytree= 0.7, learning_rate= 0.05, max_depth= 3, n_estimators= 100)\n",
    "XGB.fit(X_train_80, y_train_80)\n",
    "\n",
    "y_pred_test = XGB.predict(X_test)\n",
    "y_proba = XGB.predict_proba(X_test)\n",
    "\n",
    "\n",
    "# Compute accuracy\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "print(\"Test Accuracy: {:.5f} \".format(test_acc))\n",
    "\n",
    "# Store the Test accuracy\n",
    "XGboost_test_accuracy_test = test_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction = pd.read_csv(\"../data/model_accuracy_ibm.csv\", index_col = False)\n",
    "df_prediction =  df_prediction.drop(columns=[\"Unnamed: 0\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prediction[\"XGB_pred\"] = y_pred_test\n",
    "df_prediction[\"XGB_accuracy\"] = [XGboost_test_accuracy_test for x in range(len(y_pred_test))]\n",
    "df_prediction[\"XGB_prob\"] = y_proba[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogReg_pred</th>\n",
       "      <th>LogReg_accuracy</th>\n",
       "      <th>LogReg_prob</th>\n",
       "      <th>LGBM_pred</th>\n",
       "      <th>LGBM_accuracy</th>\n",
       "      <th>LGBM_prob</th>\n",
       "      <th>y</th>\n",
       "      <th>kNN_pred</th>\n",
       "      <th>kNN_accuracy</th>\n",
       "      <th>kNN_prob</th>\n",
       "      <th>...</th>\n",
       "      <th>SVC_pred</th>\n",
       "      <th>SVC_accuracy</th>\n",
       "      <th>SVC_prob</th>\n",
       "      <th>LSTM_price_accuracy</th>\n",
       "      <th>RNN_pred</th>\n",
       "      <th>RNN_accuracy</th>\n",
       "      <th>RNN_prob</th>\n",
       "      <th>XGB_pred</th>\n",
       "      <th>XGB_accuracy</th>\n",
       "      <th>XGB_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.499232</td>\n",
       "      <td>0</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.482038</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.517458</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>0</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.488459</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.515965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.504807</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.516175</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.517960</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.536730</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.524946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.591117</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.526401</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.526435</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.626067</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.520137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.595272</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.515859</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.527133</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.636652</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.486497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.590655</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.515859</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.526471</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.635811</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.545093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1065</th>\n",
       "      <td>1</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.543262</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.536454</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.522732</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.548477</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.430383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>1</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.570132</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.521491</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.524813</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.591784</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.592070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1067</th>\n",
       "      <td>1</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.542180</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.537546</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.522923</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.559243</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.574436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068</th>\n",
       "      <td>1</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.569741</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.521313</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>0.5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.524824</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.595539</td>\n",
       "      <td>1</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.584061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>1</td>\n",
       "      <td>0.48785</td>\n",
       "      <td>0.543188</td>\n",
       "      <td>1</td>\n",
       "      <td>0.48972</td>\n",
       "      <td>0.521491</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.508411</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.492523</td>\n",
       "      <td>0.523085</td>\n",
       "      <td>0.857009</td>\n",
       "      <td>1</td>\n",
       "      <td>0.493458</td>\n",
       "      <td>0.566151</td>\n",
       "      <td>0</td>\n",
       "      <td>0.496262</td>\n",
       "      <td>0.442955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1070 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LogReg_pred  LogReg_accuracy  LogReg_prob  LGBM_pred  LGBM_accuracy  \\\n",
       "0               0          0.48785     0.499232          0        0.48972   \n",
       "1               1          0.48785     0.504807          1        0.48972   \n",
       "2               1          0.48785     0.591117          1        0.48972   \n",
       "3               1          0.48785     0.595272          1        0.48972   \n",
       "4               1          0.48785     0.590655          1        0.48972   \n",
       "...           ...              ...          ...        ...            ...   \n",
       "1065            1          0.48785     0.543262          1        0.48972   \n",
       "1066            1          0.48785     0.570132          1        0.48972   \n",
       "1067            1          0.48785     0.542180          1        0.48972   \n",
       "1068            1          0.48785     0.569741          1        0.48972   \n",
       "1069            1          0.48785     0.543188          1        0.48972   \n",
       "\n",
       "      LGBM_prob  y  kNN_pred  kNN_accuracy  kNN_prob  ...  SVC_pred  \\\n",
       "0      0.482038  0         0      0.508411       0.5  ...         1   \n",
       "1      0.516175  1         0      0.508411       0.5  ...         1   \n",
       "2      0.526401  1         0      0.508411       0.5  ...         1   \n",
       "3      0.515859  1         1      0.508411       1.0  ...         1   \n",
       "4      0.515859  1         1      0.508411       1.0  ...         1   \n",
       "...         ... ..       ...           ...       ...  ...       ...   \n",
       "1065   0.536454  1         0      0.508411       0.5  ...         1   \n",
       "1066   0.521491  1         1      0.508411       1.0  ...         1   \n",
       "1067   0.537546  0         1      0.508411       1.0  ...         1   \n",
       "1068   0.521313  1         0      0.508411       0.5  ...         1   \n",
       "1069   0.521491  0         1      0.508411       1.0  ...         1   \n",
       "\n",
       "      SVC_accuracy  SVC_prob  LSTM_price_accuracy  RNN_pred  RNN_accuracy  \\\n",
       "0         0.492523  0.517458             0.857009         0      0.493458   \n",
       "1         0.492523  0.517960             0.857009         1      0.493458   \n",
       "2         0.492523  0.526435             0.857009         1      0.493458   \n",
       "3         0.492523  0.527133             0.857009         1      0.493458   \n",
       "4         0.492523  0.526471             0.857009         1      0.493458   \n",
       "...            ...       ...                  ...       ...           ...   \n",
       "1065      0.492523  0.522732             0.857009         1      0.493458   \n",
       "1066      0.492523  0.524813             0.857009         1      0.493458   \n",
       "1067      0.492523  0.522923             0.857009         1      0.493458   \n",
       "1068      0.492523  0.524824             0.857009         1      0.493458   \n",
       "1069      0.492523  0.523085             0.857009         1      0.493458   \n",
       "\n",
       "      RNN_prob  XGB_pred  XGB_accuracy  XGB_prob  \n",
       "0     0.488459         1      0.496262  0.515965  \n",
       "1     0.536730         1      0.496262  0.524946  \n",
       "2     0.626067         1      0.496262  0.520137  \n",
       "3     0.636652         0      0.496262  0.486497  \n",
       "4     0.635811         1      0.496262  0.545093  \n",
       "...        ...       ...           ...       ...  \n",
       "1065  0.548477         0      0.496262  0.430383  \n",
       "1066  0.591784         1      0.496262  0.592070  \n",
       "1067  0.559243         1      0.496262  0.574436  \n",
       "1068  0.595539         1      0.496262  0.584061  \n",
       "1069  0.566151         0      0.496262  0.442955  \n",
       "\n",
       "[1070 rows x 26 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_prediction.to_csv(\"../data/model_accuracy_ibm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
